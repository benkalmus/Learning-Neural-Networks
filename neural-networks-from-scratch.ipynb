{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 of the book - Activation Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function is applied to the output of a neuron. These allow neural networks to map non-linear functions. \n",
    "Some examples:\n",
    "- Step function (simplest), 0 or 1\n",
    "- Linear activation, maps x to y. Basically what we've been doing so far, passing the input of one layer to the next\n",
    "- Sigmoid y = 1/ (1 + e^-x)\n",
    "\n",
    "The problem with a step function is that it’s less clear to the optimizer what these impacts are because there’s very little information\n",
    "gathered from this function. It’s either on (1) or off (0). It’s hard to tell how “close” this step\n",
    "function was to activating or deactivating. \n",
    "\n",
    "When it comes time to optimize weights and biases, it’s easier for the\n",
    "optimizer if we have activation functions that are more granular and informative. Thats where the sigmoid comes in\n",
    "\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "import nnfs \n",
    "import matplotlib.pyplot as plt\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in the Hidden Layers \n",
    "page  85 - 93\n",
    "\n",
    "Shows a brilliant way to visualize why you need multiple hidden layers for complex behaviour. \n",
    "\n",
    "Seems like more layers allow neurons to interact with each other to produce more complex outputs, and more neurons in each layer allows you to produce more behaviours. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # weights are being factored down bcos training steps only make small changes to the weights relative to initialized values. The disproportinally large init values will cause training to take a long time\n",
    "    init_weight_multiplier = 0.01\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = self.init_weight_multiplier * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0] \n",
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "# Relu activation function\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "for i in  inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "#or numpy version\n",
    "output2 = np.maximum(0, inputs)\n",
    "\n",
    "print(f\"{output} \\n{output2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Forward pass through activation func.\n",
    "# Takes in output from previous layer\n",
    "print(dense1.output[:5])\n",
    "activation1.forward(dense1.output)\n",
    "print(activation1.output[:5])\n",
    "## all negative values have been reduced to 0, while everything else is mapped x->y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Max activation function\n",
    "\n",
    "Depending on what you require of your neural network, you may want to consider using other activation functions. \n",
    "For our example,  we'd like a classifier (to classify each branch of the spiral from spiral_data)\n",
    "\n",
    "The RELU act. func. is \n",
    "- unbounded: can be any value 0 -> inf \n",
    "- not normalized: due to ^\n",
    "- exclusive: each output is independent of the others\n",
    "\n",
    "\n",
    "To address this lack of context,\n",
    "the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and\n",
    "produce a normalized distribution of probabilities for our classes.\n",
    "This distribution returned by the softmax activation function represents **confidence scores** for each\n",
    "class and will add up to 1. \n",
    "\n",
    "![Alt text](image-2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "Normalized exponentiated values:\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "E = np.e \n",
    "\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)\n",
    "    \n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_base = sum(exp_values) # We sum all values\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('Sum of normalized values:', sum(norm_values))\n",
    "#normalized values add up to 1. This will give us a probability score for each neuron output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "[0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "# Normalize them for each sample\n",
    "probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.sum no axis \n",
      "18.172\n",
      "\n",
      "np.sum row-wise or sum of each col\n",
      "[[15.11   0.451  2.611]]\n",
      "\n",
      "np.sum col-wise or sum of each row\n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n",
      "\n",
      "np.sum same as above, but we're not keeping dimensions. Keeping dimensions aids in matrix calculations later \n",
      "[8.395 7.29  2.487]\n",
      "\n",
      "exponentiated values:\n",
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "probability values:\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Layer outputs as matrix\n",
    "layer_outputs = np.array([  [4.8, 1.21, 2.385],\n",
    "                            [8.9, -1.81, 0.2],\n",
    "                            [1.41, 1.051, 0.026]\n",
    "                            ])\n",
    "\n",
    "print(f\"np.sum no axis \\n{np.sum(layer_outputs, axis=None)}\\n\")   # no shape, just a value\n",
    "print(f\"np.sum row-wise or sum of each col\\n{np.sum(layer_outputs, axis=0, keepdims=True)}\\n\")     #.shape (1,3)\n",
    "print(f\"np.sum col-wise or sum of each row\\n{np.sum(layer_outputs, axis=1, keepdims=True)}\\n\")     #.shape (3,1)\n",
    "print(f\"np.sum same as above, but we're not keeping dimensions. Keeping dimensions aids in matrix calculations later \\n{np.sum(layer_outputs, axis=1)}\\n\")     #.shape (3,1)\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "# Normalize them for each sample\n",
    "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "print('probability values:')\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axis had to be set to 1 to act on the second dimension of layer outputs (3,3). (Axis0, Axis1 ...) to sum rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        # subracting the max of inputs. This is because the exponential function can explode! try running np.exp(1000). \n",
    "        # To prevent exp overflow, subtract the highest value from each row, which normalizes each row on the max.\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)        #probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.8 ]\n",
      " [8.9 ]\n",
      " [1.41]]\n",
      "[[1.00000000e+00 2.75983304e-02 8.93673389e-02]\n",
      " [1.00000000e+00 2.23206120e-05 1.66585811e-04]\n",
      " [1.00000000e+00 6.98374351e-01 2.50574249e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.95282664e-01, 2.47083068e-02, 8.00090293e-02],\n",
       "       [9.99811129e-01, 2.23163963e-05, 1.66554348e-04],\n",
       "       [5.13097164e-01, 3.58333899e-01, 1.28568936e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(layer_outputs, axis=1, keepdims=True))\n",
    "print(np.exp(layer_outputs - np.max(layer_outputs, axis=1, keepdims=True)))\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "softmax.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to put the lessons together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the distribution of predictions is almost equal, as each of the samples has ~33%\n",
    "(0.33) predictions for each class. This results from the random initialization of weights (a draw\n",
    "from the normal distribution, as not every random initialization will result in this) and zeroed\n",
    "biases. These outputs are also our “confidence scores.”\n",
    "\n",
    "To determine which classification the\n",
    "model has chosen to be the prediction, we perform an argmax on these outputs, which checks\n",
    "which of the classes in the output distribution has the highest confidence and returns its index - the\n",
    "predicted class index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code up to now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "[[0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "# ReLU activation      \n",
    "class Activation_ReLU:\n",
    "        # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "        keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Applying arg max returns the index of highest value from each row\n",
    "print(np.argmax(activation2.output[:5], axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Calculating Network Error with Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function, also referred to as the cost function, is the algorithm\n",
    "that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the model’s\n",
    "error, we ideally want it to be 0.\n",
    "If you’re familiar with linear regression, then you already know one of the loss functions used\n",
    "with neural networks that do regression: squared error (or mean squared error with neural\n",
    "networks).\n",
    "We’re classifying, so we need a different loss function\n",
    "\n",
    "\n",
    "Categorical cross-entropy is explicitly used to\n",
    "compare a “ground-truth” probability (y or “targets”) and some predicted distribution (y-hat or\n",
    "“predictions”), so it makes sense to use cross-entropy here.\n",
    "\n",
    "\n",
    "The formula for calculating the categorical cross-entropy of y (actual/desired distribution) and\n",
    "y-hat (predicted distribution) is:\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "Where Li denotes sample loss value, i is the i-th sample in the set, j is the label/output index, y\n",
    "denotes the target values, and y-hat denotes the predicted values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -( target_output[0] * math.log(softmax_output[0]) +\n",
    "          target_output[1] * math.log(softmax_output[1]) +\n",
    "          target_output[2] * math.log(softmax_output[2]) )\n",
    "print(loss)\n",
    "#one-hot encoding means only one value is used for calculating loss, as the other terms get multiplied by 0.  Therefore we can skip multiplication\n",
    "loss == -( target_output[0] * math.log(softmax_output[0]) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this on a numpy array of layer outputs (softmax). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "loss [0.35667494 0.69314718 0.10536052]\n",
      "average loss  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "#numpy array indexing is different to vanilla python lists. \n",
    "#we can pass an array of indicies, which we get from class_targets. we use range(len()) to index each row in softmax_outputs. \n",
    "print(softmax_outputs[ \n",
    "                      range(len(softmax_outputs)), \n",
    "                      class_targets \n",
    "                     ])\n",
    "#now convert these confidences values to loss using our equation from earlier\n",
    "loss = -np.log(softmax_outputs[ \n",
    "                               range(len(softmax_outputs)), \n",
    "                               class_targets \n",
    "                              ])\n",
    "print(f\"loss {loss}\")\n",
    "#we can do a simple analysis on loss,  such as average loss per batch\n",
    "average_loss = np.mean(loss)\n",
    "print(f\"average loss  {average_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "# one hot encoded labels\n",
    "class_targets = np.array([  [1, 0, 0],  \n",
    "                            [0, 1, 0],\n",
    "                            [0, 1, 0]])\n",
    "# Probabilities for target values -\n",
    "# only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "            softmax_outputs*class_targets,\n",
    "            axis=1\n",
    "        )\n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix log(0) = -inf problem by clipping to min and max possible confidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS_CLIP_VAL = 1e7\n",
    "# y_pred_clipped = np.clip(y_pred, LOSS_CLIP_VAL, 1 - LOSS_CLIP_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy Class Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self._forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "    \n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def __init__(self):\n",
    "        self.correct_confidences = []\n",
    "    # Forward pass\n",
    "    def _forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels  #vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            self.correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:    #matrix \n",
    "            #reminder on what this does\n",
    "            # y_pred = [[0.3, 0.7],\n",
    "            #           [0.4, 0.6]]\n",
    "            # y_true = [[0, 1],\n",
    "            #           [1, 0]] \n",
    "            # then \n",
    "            # correct_confidences = [ [0.3*0 + 0.7*1],  \n",
    "            #                         [0.4*1 + 0.6*0]]  \n",
    "            # 0 values contribute nothing, so we basically get [0.7, 0.4] (remember, we keepdims is False by default so array gets flattened)\n",
    "            # axis 1 acts on rows \n",
    "            self.correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(self.correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining our NN with Catgorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data (300, 2) | output data (300,)\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333373 0.33333296 0.33333334]\n",
      " [0.3333337  0.3333328  0.3333335 ]\n",
      " [0.3333341  0.33333236 0.33333355]\n",
      " [0.3333342  0.33333215 0.33333364]]\n",
      "loss: 1.0986164\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "print(f\"Input data {X.shape} | output data {y.shape}\")\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "# Print loss value\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
